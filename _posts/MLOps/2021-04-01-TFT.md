---
layout: post
title: "TFT를 이용한 데이터 전처리"
date: 2021-04-01
excerpt: "Tensorflow Transform을 이용하여 효율적으로 데이터 전처리를 하는 방법을 살펴본다."
tags: [MLOps]
comments: False
use_math: true
---

"좋은 Machine Learning 모델은 좋은 데이터로부터 나온다." 머신러닝을 공부, 연구하는 개발자 분들이라면 익숙하신 말일 것입니다. 그러나, 좋은 데이터를 만드는 작업은 쉬운일이 아니며, 그렇기에 Data Engineer, Data Scientist 등 너무나도 다양한 분야의 데이터 전문가들이 존재합니다.

시간이 갈수록 데이터의 중요성은 높아지고 있지만, 모델에 비해서는 확실히 각광받지 못하는 분야이기도 합니다. 실제 프로덕션을 만드는 과정에서는 더욱 중요해지는 것이 사실입니다.

따라서, 이번 시간에는 Tensorflow Extended의 subproject 중 하나인 Tensorflow Transform을 통해 전체 Machine Learning life cycle에서 어떤 위상을 차지하고 있고, 고전적인 전처리 방식에서 어떠한 점을 개선했는지 알아볼 예정입니다. 그리고 실습을 통해 독립적인 라이브러리로서의 사용 또한 알아볼 예정입니다.

## 1. ML Workflow with TFX Pipline
우선 전체적인 Machine Learning workflow를 Tensorflow Extended pipeline의 관점에서 살펴볼 필요가 있습니다. 아래 그림은 TFX pipeline인데, 주황색은 TFX의 components를, 검은색은 components가 만들어내느 artifacts를 의미합니다.

![image](https://user-images.githubusercontent.com/49096513/113210103-d460a380-92ae-11eb-974d-0509f059ccd2.png)

기존의 Tensorflow Data validation을 다룬 포스트에서 Statistics Gen, Schema Gen, Example Validator의 components를 설명하였으니 이를 참고하시면 좋을 것 같습니다.

이렇게 데이터에 대한 기초 통계량 확인 및 검증이 모두 끝나게 되면, 드디어 이번 포스트의 중점이 되는 데이터 전처리 과정을 거치게 됩니다. 
Transform component가 바로 그 역할을 수행하게 됩니다.

자세히 보시면 Transform은 input으로 두 개의 인자를 받습니다. Example Gen으로부터 생성된 artifact인 Examples와 Schema Gen으로부터 생성된 artifact인 schema가 input이 됩니다.

그리고 output으로는 saved model이 생성되고 이것은 Trainer의 input으로 들어가는 것을 확인하실 수 있습니다.

## 2. Tensorflow Transform Overview
이번엔 TFT에 대한 전반적인 개요를 조금 살펴보려 합니다. TFT는 다음과 같은 특이한 사항들이 존재하는데, 몇몇의 항목들은 조금 눈여겨 볼 필요가 있습니다. 

1. Training과 Serving의 데이터 전처리 괴리 방지
2. 전처리 단계와 ML 아티팩트를 하나의 아티팩트로 배포
3. GCP의 Dataflow 등을 활용하여 확장성 제공

조금 특이한 점은 전처리 단계와 ML 아티팩트를 하나의 아티팩트로 배포를 하게 되면서 1번의 Training, Serving 데이터 전처리 괴리를 방지할 수 있다는 점입니다.

아래의 그림은 고전적(혹은 통상적)인 model training, prediction에서 모델까지의 데이터 전달 과정입니다.
여기서 training을 할 때에는 우리가 흔히, pandas나 Apache Spark와 같은 라이브러리 등을 사용하여 데이터를 전차리 하게 됩니다. 그러나, prediction(serving)의 과정에서는 REST API 등을 통해 바로 ML model로 전달되게 되죠.

![image](https://user-images.githubusercontent.com/49096513/113209768-6c11c200-92ae-11eb-9fee-13bde15cf345.png)

이번에는 아래 그림을 통해 TFT가 어떠한 방식으로 이러한 고전적인 데이터 전처리 방식의 문제점을 해결했는지 알아봅시다. TFT는 앞서 2번에서 언급했듯, 전처리 단계와 Model을 자체의 하나의 artifact로 배포를 하면서 prediction 과정에서도 동일한 전처리를 수행할 수 있게 하였습니다.

그밖에도 TFX의 subproject이기에 더 많은 확장성을 제공하고 있다는 것이 TFT의 특징입니다.

![image](https://user-images.githubusercontent.com/49096513/113210071-c90d7800-92ae-11eb-98fe-89b9c3fe9002.png)

이번엔 본격적인 TFT의 내부 동작 방식에 대해서 개괄적으로 알아보도록 하겠습니다.

앞서 언급했듯 TFT는 두 가지의 input을 받게 됩니다. 하나는 ingest raw data이며, 하나는 해당 데이터에 대한 schema 데이터 입니다. TFT 내부에는 preprocessing_fn()이라는 함수가 존재하는데 여기서 전처리의 과정을 수행한다고 보시면 됩니다.

이렇게 수행이 끝난 데이터는 두 가지의 output으로 나오게 되는데 하나는 transform graph로 이후 saved model에 함께 배포될 아티팩트에 해당하고 다른 하나는 transformed data로 TFRecord 형식으로 Trainer의 input으로 활용되게 됩니다.

![image](https://user-images.githubusercontent.com/49096513/113210535-5224af00-92af-11eb-9737-67ba9540eb6d.png)


## 3. Usage
그렇다면 이제 어떻게 이런 것들을 활용할 수 있는지, colab을 기준으로 설명을 드리겠습니다. 

#### Installation
우선 아래 커맨드를 이용하여 설치를 진행해 줍니다. 이때 주의해야할 점은 반드시 colab을 다시 시작 해야한다는 것인데, 상단의 런타임에서 런타임 다시시작을 누르시면 됩니다.

~~~python
!pip install -q tensorflow-transform
~~~

아래 코드는 기본적인 tensorflow와 schema를 생성하기 위한 라이브러리, TFT를 실행하기 위한 apache beam 등을 불러왔습니다.
~~~python
# Schema
import tensorflow as tf
from tensorflow_transform.tf_metadata import dataset_metadata
from tensorflow_transform.tf_metadata import schema_utils

# Apache Beam
import tempfile
import tensorflow_transform.beam.impl as tft_beam
import apache_beam as beam

# TFT
import tensorflow_transform as tft
~~~

#### Raw Data
본 포스트에서 사용할 실습 데이터로 간단하게 아래와 같이 만들어 보았습니다. 가장 중점적인 것은 이것이 제대로 작동하느냐였기 때문에 그렇습니다. 작동만 된다면, 다양한 전처리를 통해 TFT가 지닌 이점을 누릴 수 있기 때문이죠.

언제나 그렇듯, 이러한 실습을 진행하실 때에는 scale-up과 다양한 데이터 예제(본인의 task에 적합한)들을 적용할 생각을 고려하면서 진행하셔야 합니다.

이번에 raw data로는 x feature에 대한 크기 3의 데이터를 준비하였습니다.

~~~python
raw_data = [{'x': 1.20}, {'x': 2.99}, {'x': 100.00}]
~~~

#### Create Schema
위에서 생성한 데이터를 바탕으로 schema를 생성하였습니다. 이렇게 생성한 schema이외에도 이전에 설명한 TFDV를 이용한 schema나 TFX 자체적으로 제공하는 Schema Gen을 활용하셔도 되고 이것이 좀 더 보편적인 경우에 해당합니다.

~~~python
raw_data_metadata = dataset_metadata.DatasetMetadata(
    schema_utils.schema_from_feature_spec({
        'x': tf.io.FixedLenFeature([], tf.float32),
    }))

raw_data_metadata
~~~

완성된 schema의 결과는 아래와 같습니다.

~~~
{'_schema': feature {
  name: "x"
  type: FLOAT
  presence {
    min_fraction: 1.0
  }
  shape {
  }
}
}
~~~

#### preprocessing_fn()
다음으로는 preprocessing_fn에 해당합니다. 이것은 하나의 함수로 전처리 내부 동작을 결정짓게 됩니다. 이번에는 간단하게 tft의 내부 함수를 활용하여 x feature를 normalized 하였습니다.

조금 유의해야할 점은 TFT에서는 전처리가 완료된 데이터에 대해 key값을 '_xf' 등을 붙여서 보다 구분이 명확하도록 하는 것을 추천하고 있습니다.

~~~python
def preprocessing_fn(inputs):
    x = inputs['x']
    x_normalized = tft.scale_to_0_1(x)
    return {
        'x_xf': x_normalized
    }
~~~


#### TFT with Apache Beam
마지막으로 Apache Beam을 사용한 TFT입니다. 본 포스트에서는 apache beam이 메인이 아니기 때문에 구체적인 설명은 생략하도록 하겠습니다.

TFX는 오케스트레이터로 Apache Beam이외에도 Apache Airflow나 Kubeflow 등을 사용합니다.

~~~python
with beam.Pipeline() as pipeline:
    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):
      
        transformed_dataset, transform_fn = (
            (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(
                preprocessing_fn))

transformed_data, transformed_metadata = transformed_dataset
print(transformed_data)
~~~

전처리가 완료된 결과이고, 자세히 보시면 최소값은 0 최대값은 1로 잘 설정된 것을 확인하실 수 있습니다. 
~~~
[{'x_xf': 0.0}, {'x_xf': 0.018117407}, {'x_xf': 1.0}]
~~~

## 4. Reference
대부분의 것들은 아래의 책과 TFX의 예제를 참고하였습니다. 
* “Building Machine Learning Pipelines”, Hannes Hapke
* TFX Examples
